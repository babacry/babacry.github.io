---
title: Chapter3 Pursuit Algorithms-Practice
layout: post
category : [books, Sparse and Redundant Representation] 
tagline: "Supporting tagline"
tags : [sparse, image processiong, compressed sensing]
---
This chapter concerns about the optimization problem arised earlier.  
\\[(P_0): min_x \lVert \boldsymbol{x}\rVert_0\,\, subject\, to\, \boldsymbol{b} = \boldsymbol{A}_{n\times m}\boldsymbol{x}\\]   
The book show two methods concerns two properties of this problem.  

1. Support of x is disceret: Greedy Algorithm  
2. Smoothing the penalty function $$\lVert x \rVert_0$$: The relaxation methods

## 1. Greedy Algorithms  

### 1.1. Core Idea  

#### Example 1.  
Spark($$A$$) > 2, val($$P_0$$) = 1.

* Test column by column s.t. $$\boldsymbol{a}_j//\boldsymbol{b}$$.
* Time: O(mn).
* Let $$\epsilon (j) = \lVert \boldsymbol{a_j}z_j-\boldsymbol{b}\rVert_2$$ be the error,  
to get the minimizing of $$\epsilon (j) \quad z_j^{\ast} = \frac{a_j^T \boldsymbol{b}}{\lVert \boldsymbol{a}_j\rVert_2^2}$$   
$$min \epsilon (j) = \epsilon (j) {\lvert}_{z_j = z_j^{\ast}} = \lVert \boldsymbol{b} \rVert_2^2 - \frac{(\boldsymbol{a}_j^T \boldsymbol{b})^2}{\lVert \boldsymbol{a}_j\rVert_2^2}$$  
Actually, $$\lvert \boldsymbol{a}_j^T b \rvert \le \lVert \boldsymbol{a}_j \rVert_2 \lVert\boldsymbol{b}\rVert_2$$ "=" holds when $$\boldsymbol{a}_j // \boldsymbol{b}_{}$$

### 1.2. The Orthogonal-Matching-Pursuit && 1.3. Other Greedy Methods  

$$\S$$ 1.2 introduces OMP and LS-OMP algorithm. $$\S$$ 1.3 introduces MP and Weak-MP

* OMP: Orthogonal-Matching-Pursuit  
* LS-OMP: Least-Square Orthogonal-Matching-Pursuit
* MP: Matching-Pursuit 
* Weak-MP: Weak-Matching-Pursuit 

||OMP|LS-OMP|MP|Weak-MP| 
|:---|:---|:---|:---|:---|  
|**Init**|$$\boldsymbol{x}^0=0 \\ \boldsymbol{r}^0 =\boldsymbol{b}-\boldsymbol{Ax}^0 = \boldsymbol{b} \\ \mathcal{S}^0 = \varnothing$$  |$$\boldsymbol{x}^0=0 \\ \boldsymbol{r}^0 =\boldsymbol{b}-\boldsymbol{Ax}^0 = \boldsymbol{b} \\ \mathcal{S}^0 = \varnothing$$ |$$\boldsymbol{x}^0=0 \\ \boldsymbol{r}^0 =\boldsymbol{b}-\boldsymbol{Ax}^0 = \boldsymbol{b} \\ \mathcal{S}^0 = \varnothing$$ |$$\boldsymbol{x}^0=0 \\ \boldsymbol{r}^0 =\boldsymbol{b}-\boldsymbol{Ax}^0 = \boldsymbol{b} \\ \mathcal{S}^0 = \varnothing$$| 
|**Sweep**|$$\epsilon (j) = min_{z_j} \lVert \boldsymbol{a}_jz_j-\boldsymbol{r}^{k-1} \rVert_2^2\\ (j \notin \mathcal{S}^{k-1})$$|$$\epsilon (j) = min_{\boldsymbol{x}} \lVert \boldsymbol{Ax-b}\rVert_k^2 \\ s.t. \,supp\{\boldsymbol{x}\} = \mathcal{S}^{k-1}\cup \{j\}$$|$$\epsilon (j) = min_{z_j} \lVert \boldsymbol{a}_jz_j-\boldsymbol{r}^{k-1} \rVert_2^2\\ (j = 1,2,3,\dots,m)$$|  $$\epsilon (j) = min_{z_j} \lVert \boldsymbol{a}_jz_j-\boldsymbol{r}^{k-1} \rVert_2^2\\ for\, j = 1,2,3,\dots\\until \,\lVert \boldsymbol{r}^{k-1}\rVert_2^2\cdot t \le \frac{(\boldsymbol{a}_j^T \boldsymbol{r}^{k-1})^2}{\lVert \boldsymbol{a}_j\rVert_2^2}$$|
|**Update Support**|$$\mathcal{S}^k = \mathcal{S}^{k-1} \cup \{j_0\} \\ s.t. \forall j \notin \mathcal{S}^{k-1} \epsilon (j_0) \le \epsilon (j)$$|$$\mathcal{S}^k = \mathcal{S}^{k-1} \cup \{j_0\} \\ s.t. \forall j \notin \mathcal{S}^{k-1} \epsilon (j_0) \le \epsilon (j)$$|$$\mathcal{S}^k = \mathcal{S}^{k-1} \cup \{j_0\} \\ s.t. \forall j \notin \mathcal{S}^{k-1} \epsilon (j_0) \le \epsilon (j)$$|$$\mathcal{S}^k = \mathcal{S}^{k-1} \cup \{j_0\}$$ ($$ j_0$$ is the stop j of sweep)|
|**Update Provisional Solution**|$$\boldsymbol{x}^k\, s.t.\,\lVert \boldsymbol{Ax-b}\rVert_k^2 \\ subject\, to\, supp\{\boldsymbol{x}\} = \mathcal{S}^k$$|$$\boldsymbol{x}^k\, s.t.\,\lVert \boldsymbol{Ax-b}\rVert_k^2 \\ subject\, to\, supp\{\boldsymbol{x}\} = \mathcal{S}^k$$|$$1. \boldsymbol{x}^k = \boldsymbol{x}^{k-1}\\ 2. \boldsymbol{x}^k(j_0) = \boldsymbol{x}^k(j_0) z_{j_0}^{\ast}$$|$$1. \boldsymbol{x}^k = \boldsymbol{x}^{k-1}\\ 2. \boldsymbol{x}^k(j_0) = \boldsymbol{x}^k(j_0) z_{j_0}^{\ast}$$|
|**Update Residue**|$$\boldsymbol{r}^k = \boldsymbol{Ax}^k-\boldsymbol{b}$$|\$$\boldsymbol{r}^k = \boldsymbol{Ax}^k-\boldsymbol{b}$$| $$\boldsymbol{r}^k = \boldsymbol{Ax}^k-\boldsymbol{b}$$|$$\boldsymbol{r}^k = \boldsymbol{Ax}^k-\boldsymbol{b}$$|  
|**Stopping Rule**|$$\lVert\boldsymbol{r}^k \rVert_2 < \epsilon_0$$|$$\lVert\boldsymbol{r}^k \rVert_2 < \epsilon_0$$|$$\lVert\boldsymbol{r}^k \rVert_2 < \epsilon_0$$|$$\lVert\boldsymbol{r}^k \rVert_2 < \epsilon_0$$|   

### 1.4 Normalization  

$$\widetilde{\boldsymbol{A}} = \boldsymbol{AW} \quad \boldsymbol{W} = diag(\frac{1}{\lVert \boldsymbol{a}_1 \rVert_2}, \frac{1}{\lVert \boldsymbol{a}_2 \rVert_2}, \dots, \frac{1}{\lVert \boldsymbol{a}_n \rVert_2})$$

**Theorem 3.1** The greedy algorithms(OMP, MP and Weak-MP) produce same solution support $$\mathcal{S}^k$$ when using the original matrix $$\boldsymbol{A}$$ or its normalized version $$\widetilde{A}$$.   

**Proof.**

* OMP:  

    1. Assume $$\boldsymbol{r}^k$$ the same.  
    $$\epsilon (j) = \lVert \boldsymbol{r}^{k-1} \rVert_2^2 - \frac{(\boldsymbol{a}_j^T \boldsymbol{r}^{k-1})^2}{\lVert \boldsymbol{a}_j\rVert_2^2} = \lVert \boldsymbol{r}^{k-1} \rVert_2^2 -\frac{(\widetilde{\boldsymbol{a}_j}^T \boldsymbol{r}^{k-1})^2}{\lVert \widetilde{\boldsymbol{a}_j}\rVert_2^2} $$ 
    So, we have the same $$j_0$$   
    2. Then    
    $$ 
    \begin{array}{lll}
    \widetilde{\boldsymbol{r}^k} & = & [\boldsymbol{I} - \widetilde{\boldsymbol{A}}_{\mathcal{s}^k}(\widetilde{\boldsymbol{A}}_{\mathcal{s}^k}^T\widetilde{\boldsymbol{A}_{\mathcal{s}^k}}^{-1}\widetilde{\boldsymbol{A}}_{\mathcal{s}^k}^T]\boldsymbol{b} \\
    & = & [\boldsymbol{I} - \boldsymbol{A}_{\mathcal{s}^k} \boldsymbol{W} (\boldsymbol{W}^T \boldsymbol{A}_{\mathcal{s}^k}^T \boldsymbol{A}_{\mathcal{s}^k}\boldsymbol{W})^{-1}\boldsymbol{A}_{\mathcal{s}^k}^T]\boldsymbol{b} \\
    & = & [\boldsymbol{I} - \boldsymbol{A}_{\mathcal{s}^k} (\boldsymbol{A}_{\mathcal{s}^k}^T \boldsymbol{A}_{\mathcal{s}^k})^{-1}\boldsymbol{A}_{\mathcal{s}^k}^T]\boldsymbol{b} \\
    & = & \boldsymbol{r}^k
    \end{array}
    $$  
    We have the same residue.
* MP:

    1.  Get the same $$j_0$$ like OMP
    2.  Assume $$\widetilde{\boldsymbol{r}^{k-1}} = \boldsymbol{r}^{k-1}$$  
    $$\widetilde{\boldsymbol{r}}^k = \boldsymbol{r}^{k-1} - \frac{(\widetilde{\boldsymbol{a}}_{j_0}^T \widetilde{\boldsymbol{r}}^{k-1})^2}{\lVert \widetilde{\boldsymbol{a}}_{j_0}\rVert_2^2}\widetilde{\boldsymbol{a}}_{j_0} = \boldsymbol{r}^{k-1}- \frac{(\boldsymbol{a}_{j_0}^T \boldsymbol{r}^{k-1})^2}{\lVert \boldsymbol{a}_{j_0}\rVert_2^2} \boldsymbol{a}_{j_0} = \boldsymbol{r}^k $$

* Weak-MP: 

    1. We have $$ \frac{\lvert\boldsymbol{a}_{j_0}^T \boldsymbol{r}^{k-1}\rvert}{\lVert \boldsymbol{a}_{j_0}\rVert_2} \ge t \cdot \lVert\boldsymbol{r}^{k-1} \rVert_2 \Leftrightarrow \lvert\widetilde{\boldsymbol{a}}_{j_0}^T \boldsymbol{r}^{k-1}\rvert \ge t \cdot \lVert\boldsymbol{r}^{k-1} \rVert_2 $$   
    2. Same as MP 

* Normalization make things easier  
* Remember $$\boldsymbol{x}^k = \boldsymbol{W\widetilde{x}}^k$$

### 1.5. Rate of Decay of the Residual in Greedy Methods  

* Consider $$\lVert \boldsymbol{r}^k \rVert_2^2$$ with $$\lVert \boldsymbol{r}^{k-1} \rVert_2^2$$ 
* Assume normalized

In MP methods, we have $$\boldsymbol{r}^k = \boldsymbol{r}^{k-1}-z_{j_0}^{\ast}\boldsymbol{a}_{j_0} = \boldsymbol{r}^{k-1} - (\boldsymbol{a}_{j_0}^T\boldsymbol{r}^{k-1})\boldsymbol{a}_{j_0}$$  
So, $$\lVert \boldsymbol{r}^k \rVert_2^2 = \lVert \boldsymbol{r}^{k-1} \rVert_2^2 - max_{1\le j\le m} (\boldsymbol{a}_{j_0}^T\boldsymbol{r}^{k-1})^2$$ 

**Definition 3.1 (Decay factor $$\delta (A, v)$$)**   
$$\boldsymbol{A}$$ is normalized, $$v$$ is an arbitrary vector.  
Then $$\delta (A) = max_{1\le j\le m} \frac{\lvert \boldsymbol{a}_j^T\boldsymbol{v}\rvert}{\lVert \boldsymbol{v} \rVert_2^2}$$

**Definition 3.2 (Universal factor $$\delta (A)$$)**  
\boldsymbol{A} is normalized matrix.  
Then $$\delta (A) = inf_{\boldsymbol{v}} max_{1\le j\le m} \frac{\lvert \boldsymbol{a}_j^T\boldsymbol{v}\rvert}{\lVert \boldsymbol{v} \rVert_2^2} = inf_{\boldsymbol{v}}\delta (A, v)$$ 

With the above definition, we have  
\\[
\lVert \boldsymbol{r}^k \rVert_2^2 = \lVert \boldsymbol{r}^{k-1} \rVert_2^2 (1-\delta (\boldsymbol{A})) 
\\]
So, we have $$\lVert \boldsymbol{r}^k \rVert_2^2\le (1-\delta (\boldsymbol{A}))^k \lVert \boldsymbol{b} \rVert_2^2 $$  

* OMP: $$\lVert \boldsymbol{r}^k \rVert_2^2\lvert_{OMP} \le \lVert \boldsymbol{r}^k \rVert_2^2\lvert_{MP}$$
* Weak-MP: decay slower, since $$\lVert \boldsymbol{r}^k \rVert_2^2\le (1-t^2\delta (\boldsymbol{A})) \lVert \boldsymbol{r}^{k-1} \rVert_2^2 $$  

So, the greedy methods ensure convergence.  

**An alternative expression of $$\delta (A)$$**  
Actually, $$\delta (A) = inf_{\boldsymbol{v}} \frac{\lVert \boldsymbol{Av} \rVert_{\infty}^2}{\lVert \boldsymbol{v} \rVert_2^2}$$.  
Replace $$l_{\infty}$$-norm with $$l_2$$-norm, then $$\delta (\boldsymbol{A}) = \lambda_{min} {}_{}$$

### 1.6. Thresholding Algorithm  

Given the number of atoms desired: k.

1. Get $$\epsilon (j) = min_{z_j} \lVert \boldsymbol{a}_jz_j-\boldsymbol{r}^{k-1} \rVert_2^2 \quad (j = 1,2,3, \dots, m)$$
2. Find the 1-k smallest error, combine these index as $$\mathcal{S}$$
3. Caculate $$min_{\boldsymbol{x}} \lVert \boldsymbol{Ax}-\boldsymbol{b}\rVert_2^2 \quad s.t. \, supp\{x\} = \mathcal{S}$$  

### 1.7. Numerical Demonstration of Greedy Algorithms  


## Convex Relaxation Techniques  

### 3.2.1 Relaxation of $$l_0$$-norm

* $$l_0$$-norm is highly discontinuous. 

Replace with $$l_p$$-norm $$(0 < p \le 1)$$ or smooth functions.

\\[
(M_k): \quad min_{\boldsymbol{x}} \lVert \boldsymbol{X}_{k-1}^{\dagger} \boldsymbol{x} \rVert_2^2 \quad subject \, to \, \boldsymbol{b} = \boldsymbol{Ax}
\\]
$$
\mathcal{L}(\boldsymbol{x}) = \lVert \boldsymbol{X}_{k-1}^{\dagger} \boldsymbol{x} \rVert_2^2 - \boldsymbol{\lambda}^T(\boldsymbol{b}-\boldsymbol{Ax})\\
\frac{\partial{\mathcal{L}(\boldsymbol{x})}}{\partial{\boldsymbol{x}}} = 0 = 2(\boldsymbol{X}_{k-1}^{\dagger})^2 \boldsymbol{x}-\boldsymbol{A}^T\boldsymbol{\lambda} \\
\boldsymbol{x}_k = \frac{1}{2}   \boldsymbol{X}_{k-1}^2\boldsymbol{A}^T\boldsymbol{\lambda}\\
\boldsymbol{Ax} = b \Rightarrow \boldsymbol{\lambda} = 2 
$$
